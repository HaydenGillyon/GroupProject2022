# AI

## Problem Solving as Search

### Problem Solving

- Problems are represented as graphs
- Problem solving corresponds to searching a graph

### A problem from blocks world




![image-20220201112101785](/home/jojo/Documents/UNI/term2/images/image-20220201112101785.png)

![image-20220201112151166](/home/jojo/Documents/UNI/term2/images/image-20220201112151166.png)

#### Problem Formulation as State Space Search

- State space = Directed graph
- Nodes ~ Problem situations
- Arcs ~ Actions , legal moves
- Problem = ( State space , Start , Goal condition)
- Note : several nodes may satisfy goal condition
- Solving a problem ~ Finding a path
- Problem solving ~ Graph search
- Problem solution ~ Path from start to a goal node

#### State Spaces for Optimisation Problems

- Optimisation : minimise cost of solution

- In blocks world:

  ​	actions may have different costs. (blocks may have different weights)

- Assign costs to arcs

- Cost solution = cost of solution path

### More Realistic Examples

- Making a time table
- Production scheduling 
- Grammatical parsing
- Interpretation of sensory data
- Modelling from measured data
- Finding scientific theories that account for experimental data

### Search Methods

- Uninformed techniques: systematically search complete graph , unguided
- Informed methods : Use problem specific information to guide search in promising directions.

### Uninformed Search

- Depth - first search
- Breadth  - first search
- Iterative deepening
- Bi - directional search

Evaluation of Search strategies

- completeness: does it always find a solution if one exists?
- time complexity: number of nodes generated
- space complexity: maximum number of nodes in memory
- optimality: does it always find a least -cost solution?

### Depth  - First Search

![image-20220201113403838](/home/jojo/Documents/UNI/term2/images/image-20220201113403838.png)

- Completeness: No, susceptible to infinite loops (should check for cycles)
- Time Complexity: HIGH time complexity as in the worst case visits all states
- Space Complexity: LOW space complexity as it remembers only one path at each time
- Optimality: NO , not guaranteed to find shortest solution first.

![image-20220201113606307](/home/jojo/Documents/UNI/term2/images/image-20220201113606307.png)

### Iterative Deepening Search

- Dept. - limited search may miss a solution if depth-limit is set too low.
- This may be problematic if solution length not known in advance
- Idea : start with small MaxDepth and increase MaxDepth until solution found

### Properties of Iterative Deepening Search

- Completeness: YES as it avoids loops
- Time Complexity: HIGH  time complexity as in the worst case visit all states multiple times.
- Space Complexity: LOW space complexity as it remembers only one path at each time.
- Optimality: YES as it finds the shortest solution first

![image-20220201114119835](/home/jojo/Documents/UNI/term2/images/image-20220201114119835.png)

- Breadth-first search completes one level before moving on to next level
- Has to keep in memory all the competing paths that aspire to be extended to a goal node.

### Properties of Breath - First Search

- Completeness : Yes
- Time Complexity: HIGH time complexity as in the worst case visit all states
- Space Complexity: HIGH space complexity as it remembers all paths
- Optimality: YES

### Complexity of Basic Search Methods

- For a simpler analysis consider state-space as a tree
- Uniform branching b
- Solution at depth d

![image-20220201114819124](/home/jojo/Documents/UNI/term2/images/image-20220201114819124.png)

![image-20220201114915015](/home/jojo/Documents/UNI/term2/images/image-20220201114915015.png)

![image-20220201114925906](/home/jojo/Documents/UNI/term2/images/image-20220201114925906.png)

### Time & Space Complexity

- Breadth - first and iterative deepening guarantee shortest solution
- Breadth - first : high space complexity
- Depth  - first : low space complexity , but may search well below solution depth
- Iterative deepening: best performance in terms of orders of complexity

### Time & Space Complexity of Iterative Deepening 

- Repeatedly re-generates upper levels nodes
- Start node (level 1 ): d times
- Level 2: (d-1) times
- Level 3: (d-2) times
- Notice : Most work done at last level d , typically than at all previous levels

### Time Overheard of Iterative Deepening

- Example: binary tree, d = 3 , #nodes = 15

- breadth-first generates 15 nodes

- Iter . deepening: 26 nodes

- Relative overheads due to re-generation : 26/15

- Generally

  ~~~
  nodes generated by iter.deep             b   
  /                                  <     /     
  nodes generated by breadth-first         b-1
  ~~~

  

### Backward Search

  - Search from goal to start
  - New goal condition satisfied by start node 
  - Only applicable if original goal node(s) known 
  - Depends on branching in forward/backward direction

  ### Bidirectional Search

- Search progresses from both start and goal

- Standard search techniques can be used on re-defined state space

- Problem situations defined as pairs of form:

  StartNode -Goal Node

  ![image-20220201141041409](/home/jojo/Documents/UNI/term2/images/image-20220201141041409.png)

## Evolutionary Algorithms

### Optimisation problem

![image-20220208133717720](/home/jojo/Documents/UNI/term2/images/image-20220208133717720.png)

### Optimisation Problem

- Types of problems:
  - Continuous 
  - Combinatorial
- Problem difficulty:
  - Solvable analytically (calculus , linear algebra)
  - Solvable using mathematical optimisation (convex optimisation problem)
  - Unsolvable with traditional methods (multi - modal problems)

### Local search algorithm

start problem inside of search space , e.g. act like an ant trying to find food in the current space

• In many optimization problems, the path to the goal is irrelevant; the goal state itself is the solution

• State space = set of "complete" configurations • Find configuration satisfying constraints, e.g., n- queens

 keep a single "current" state, try to improve it

![image-20220208134452751](/home/jojo/Documents/UNI/term2/images/image-20220208134452751.png)

![image-20220208134531234](/home/jojo/Documents/UNI/term2/images/image-20220208134531234.png)

![image-20220208134809537](/home/jojo/Documents/UNI/term2/images/image-20220208134809537.png)

![image-20220208135203779](/home/jojo/Documents/UNI/term2/images/image-20220208135203779.png)

![image-20220208135259113](/home/jojo/Documents/UNI/term2/images/image-20220208135259113.png)

### Properties of simulated annealing

- One can prove : if T decreases slowly enough , then simulated annealing search will find a global optimum with probability approaching 1.

![image-20220208135729812](/home/jojo/Documents/UNI/term2/images/image-20220208135729812.png)

![image-20220208140213103](/home/jojo/Documents/UNI/term2/images/image-20220208140213103.png)

### Necessary Ingredient's for EA to Work

- A way to represent solutions (pheno types) are strings of symbols (genotypes)
- A function that maps to genotypes to phenotypes and maps phenotypes to a measure of 'fitness'
- Operators to reproduce and vary individual genotypes in such a way that inheritance of trait occurs
- The EA does need to know details of the fitness . fitness function can be a 'black box'

![image-20220208140703367](/home/jojo/Documents/UNI/term2/images/image-20220208140703367.png)

### Function Maximisation

Evolutionary approaches suitable for not well behaved functions

- Represent candidate solution directly as real numbers

- Use mutation and crossover on real numbers

  - Crossover: randomised average of parents
  - Mutation : small random perturbation

  

### Search in Games

#### Game Types

|                | deterministic                   | chance                    |
| -------------- | ------------------------------- | ------------------------- |
| perfect info   | chess , checkers , go , othello | backgammon monopoly       |
| imperfect info | battleships / tic tac toe       | bridge , poker , scrabble |

### Games vs. Search Problems



- Unpredictable opponent -> specifying a move for every possible opponent reply
- Time limits -> unlikely to find goal , must approximate

![image-20220210164048750](/home/jojo/Documents/UNI/term2/images/image-20220210164048750.png)

#### Mini Max Algorithm 

- General method for determining optimal move
- Generate complete game tree down to terminal states
- Compute utility of each node bottom up from leaves toward root.
- At each Max node , pick the move with max utility
- At each Min node,  pik the move with min utility 
- When reach the root , optimal move is determined

All possibilities are computed

![image-20220210164612124](/home/jojo/Documents/UNI/term2/images/image-20220210164612124.png)

![image-20220210165511017](/home/jojo/Documents/UNI/term2/images/image-20220210165511017.png)

#### Imperfect Decision

- Generating the complete game tree for all but the simplest games is intractable.

- Instead , cut off search at some nodes and estimate expected utility using a heuristic evaluation function.

- heuristic measures the probability that max will win given a position characterized by a given set of features , hopefully

- Sample chess evaluation function based on material advantage , pawn = 1 , knight/bishop = 3 , rook =5 , queen = 9.

- An example of a weighted linear function:

  ![image-20220210165901351](/home/jojo/Documents/UNI/term2/images/image-20220210165901351.png)

![image-20220210165933420](/home/jojo/Documents/UNI/term2/images/image-20220210165933420.png)

![image-20220210170102474](/home/jojo/Documents/UNI/term2/images/image-20220210170102474.png)

#### Alpha - Beta Pruning

- Frequently , large parts of the search space are irrelevant to the finanl decision and can be pruned
- No need to explore options that are already definitely worse than the current best options.

##### The Procedure

- The alpha-beta procudure can speed up depth-first minimax search
- Alpha : lower bound on the value of MAX node
- v >= a
- Beta: upper bound on the value of MIN node
- v<=b

![image-20220210170751559](/home/jojo/Documents/UNI/term2/images/image-20220210170751559.png)

![image-20220210170824723](/home/jojo/Documents/UNI/term2/images/image-20220210170824723.png)

![image-20220210170921468](/home/jojo/Documents/UNI/term2/images/image-20220210170921468.png)

##### Properties of a-b testing

- Pruning does not affect final result . This means that it gets the exact same result as doing full minmax
- Good move ordering improves effectiveness of pruning
- With "perfect ordering" , time complexity = O(b^m/2)

#### Shallow Search techniques

- limited search for a few levels
- reorder the level - l succesors
- proceed with a - b minimax search

### Additional Refinements

- Waiting for Quiescence: continue the search until no drastic change occurs from one level to the next.
- Secondary Search: after choosing a move, search a few more levels beneath it to be sure it still looks good.
- Book moves: for some parts of the game , keep a catalogue of best moves to make.

![image-20220210171838476](/home/jojo/Documents/UNI/term2/images/image-20220210171838476.png)

## Knowledge Representation

What is knowledge ?

In conversational terms it means facts about the world

#### Use of Knowledge in CS

|               Context                |                  Use of Knowledge                   |
| :----------------------------------: | :-------------------------------------------------: |
|               Database               | Store data about real-world systems, answer queries |
|               Internet               |     Information on anything at your fingertips      |
|             Programming              |        Tell computers how to perform a task         |
|      Problem - solving , search      |      Heuristic knowledge for efficient search       |
| Natural language conversation (Siri) |               Common sense knowledge                |
|        Expert advice (doctor)        |       Domain knowledge of a specific subject        |
|                                      |                                                     |

- What is knowledge representation?
  - What is representation? • Representation refers to a symbol or thing which represents ( `refers to '  , ' stands for' ) something else͘.
- When do we need to represent?
  - represent a thing of the natural world when there is no possibility to use the original thing.

What is reasoning?

- Knowledge representation is used to enable an entity to determine consequences by reasoning about the world, rather than acting in the world.

- Reasoning is the use of symbolic representations of some statements about the world in order to derive new ones , predictions and consequences

- Reasoning can be as easy as mechanical symbol manipulation

### Three Aspects of KR

- **Syntactic**

  - Possible (allowed constructions

  - Each individual representation is often called a sentence.

    For example: color(my_car, red), my_car(red), red(my_car), etc.

- **Semantic**

  - What does the representation mean (maps the sentences to the world
  - For example : color(my_car͕ redͿ ї ͍͍ ‘my car is red͛, ‘paint my car red͛, etc.

- **Inferential** 

  - The interpreter
  - Decides what kind of conclusions can be drawn
  - For Example : Modus ponens (P, P -> Q , therfore Q)

### Well - defined syntax / semantics

- **Precise** syntax and semantics of the representation + correct reasoning guarantee that new conclusions correspond to true facts in the real world

![image-20220215133538593](/home/jojo/Documents/UNI/term2/images/image-20220215133538593.png)  

### Procedural vs Declarative Knowledge

![image-20220215133607113](/home/jojo/Documents/UNI/term2/images/image-20220215133607113.png)

### Requirements for KR languages

- Representation of adequacy
  - should allow representing all the required knowledge
- Inferential adequacy
  - should allow inferring new knowledge
- Inferential efficiency
  - inferences should be efficient
- Clear syntax and semantics
  - unambiguous and well - defined syntax and semantics 
  - Naturalness
    - easy to read and use

Representations of brain

- At a neural level in the electric activity  can be represented as  -
  - Artificial Neural Networks
- The internal language used to think -
  - Logics
- If - then rules
  - Production rules
- Associations and relations about concepts
  - Semantic Networks
- Prototypical situations & scripts
  - Frames

### What is a Logic?

• A language with
unambiguous rules
• No ambiguity in representation (may be other errors!)
• Allows unambiguous communication and processing
• Very unlike natural languages e.g. English

Not to be confused with logical reasoning
• Logics are languages, reasoning is a process (may use logic)

### Propositional Logic

Syntax
• Propositions͕ e͘g͘ P= "t is wet"
• Connectives: and, or, not, implies, iff (equivalent)

![image-20220215134504369](/home/jojo/Documents/UNI/term2/images/image-20220215134504369.png)

• Brackets, T (true) and F (false)
• Semantics
• Define how connectives affect truth
• ͞P and Q͟ is true if and onlǇ if P is true and Q is true
• Use truth tables to work out the truth of statements

### First Order Logic 

![image-20220215134903638](/home/jojo/Documents/UNI/term2/images/image-20220215134903638.png)

![image-20220215134941705](/home/jojo/Documents/UNI/term2/images/image-20220215134941705.png)

### Beyond True and False

- Three valued logic: true , false & unknown
- Fuzzy logic : degree of membership [0,1] , e.g. , how much a given temperature can be classified as "hot"? It models imprecision
- Bayesian logic : degree of confidence [0,1] , e.g. how much do I believe that unicorns exists? It models subjective belief

### Logic : pros & cons

- Pros:

  - Fairly easy to translate natural language to logic , when possible
  - Well studied : branches of mathematics devoted to it
  - It comes with logical reasoning "for free"
  - Implemented in programming languages (Prolog)

- Cons:

  - Can be hard to work with ( for non logicians)

  - Not natural representation, as people do not think in first order logic

  - "too" declarative , inference is computationally inefficient

-> NON - LOGICAL REPRESENTATIONS

### Production Rules

- Rule set of <condition, action> pairs
  - "if condition then action"
- Match - resolve - act cycle
  - Match : Agent checks if each rule's condition holds
  - Resolve:
    - Multiple production rules may fire at once (conflict set)
    - Agent must choose rule from set (conflict resolution)
    - Act͗ If so͕ rule ͞fires͟ and the action is carried out
- • Working memory:
  • rule can write knowledge to working memory
  • knowledge may match and fire other rules

### Production Rules Example

• IF (at bus stop AND bus arrives) THEN action(get on
the bus)
• IF (on bus AND not paid AND have oyster card) THEN
action(pay with oyster) AND add(paid)
• IF (on bus AND paid AND empty seat) THEN action(sit
down)
• Conditions and actions must be clearly defined
• Reasoning = Firing Rules
• Can easily be expressed in first order logic

### Semantic networks : syntax

- Represent as a graph
- Nodes represent concepts , actions or objects in the world
- Links represent directional and labelled relationships bertween nodes
  - Inheritance - oriented links : "isa" , "instance_of "
  - General links: "has_part", "causes"
  - Domain - specific links: "has_disease", "father_of"
- Reasoning = inheritance of properties between nodes

### Frame Representation

- Semantic networks where nodes have structure
  - Frame with a number of slots
  - Each slot stores specific item of information
- When agent faces a new situation
  - Slots can be filled in (value may be another frame)
  - Filling in may trigger actions
  - May trigger retrieval of other frames
- Inheritance of properties between frames
  - Very similar to objects in OOP.

![image-20220215140518782](/home/jojo/Documents/UNI/term2/images/image-20220215140518782.png)

### Flexibility in Frames

- Slots in a frame can contain
  - Information for choosing a frame in a situation
  - Relationships between this and other frames
  - Procedures to carry out after various slots filled
  - Default information to use where input is missing
  - Blank slots: left blank unless required for a task
  - Other frames, which gives a hierarchy

### Representation & Logic

 AI wanted "non-logical representations͟"
	• Production rules
	• Semantic networks
	•Frames

• But all can be expressed in first order logic!
• Best of both worlds
	• Logical reading ensures representation well-defined
	• Representations specialised for applications
	• Can make reasoning easier, more intuitive

### Applications

- Knowledge-based problem solving
- Expert systems
- Natural language conversations
- Automatic reasoning/ theorem proving
- Semantic Web.

### Logic In general

- Logic are a formal language for representing information such that conclusions can be drawn

- Syntax defines the sentences in the languages

- Semantics define the "meaning of sentences

  ![image-20220217134740551](/home/jojo/Documents/UNI/term2/images/image-20220217134740551.png)

  

### Propositional logic

Propositional logic is simplest logic

**Defintion** : a proposition is a statement that is either true or false.

### Examples

![image-20220217134846912](/home/jojo/Documents/UNI/term2/images/image-20220217134846912.png)

### Propositional logic syntax

Symbols:

- logical constants: True , False
- propositional symbols P,Q
- Logical connectives :

![image-20220217135031018](/home/jojo/Documents/UNI/term2/images/image-20220217135031018.png)

- Parenthesis

![image-20220217135057883](/home/jojo/Documents/UNI/term2/images/image-20220217135057883.png)

### Truth tables for connectives

![image-20220217163642284](/home/jojo/Documents/UNI/term2/images/image-20220217163642284.png)

### Translation

Translation of English sentences to propositional logic:

1.  identify atomic sentences that are propositions 
2.  Use logical connectives to translate more complex composite sentences that consist of many atomic senteencces

#### Assume the following sentence:

- It is not sunny this afternoon and it is colder than yesterday

Atomic sentences

- p = It is sunny this afternoon
- q =  it is colder than yesterday

Translation : ¬ p ^ q

Translation

Assume the following sentences:

![image-20220217164110197](/home/jojo/Documents/UNI/term2/images/image-20220217164110197.png)

### Logical equivalence

![image-20220217164139756](/home/jojo/Documents/UNI/term2/images/image-20220217164139756.png)

### Validity and Satisfiability

A sentence is **valid** (tautology) if it is true in **all** models

![image-20220217164247420](/home/jojo/Documents/UNI/term2/images/image-20220217164247420.png)

A sentence is satisfiable if it is true in some model

A sentence is unsatisfiable (contradiction) if it is true in no models

![image-20220217164449539](/home/jojo/Documents/UNI/term2/images/image-20220217164449539.png)

### Inference

![image-20220217165024869](/home/jojo/Documents/UNI/term2/images/image-20220217165024869.png)

### Inference methods

- How to design a procedure that given KB and a determines answer the question : KB |= a

- Inference procedures are search procedures

- Proof methods divide into two kinds:

  - Application of inference rules

    - Legitimate generation of new sentences from old
    - Proof  = a sequence of inference rule applications
    - Can use inference  rules as operators in a standard search algorithm
    - Typically require transformation of sentences into a normal form

  - Model checking

    - Truth table enumeration

    - Backtracking

    - Heuristic search in model space

      

![image-20220217170225505](/home/jojo/Documents/UNI/term2/images/image-20220217170225505.png)

### Resolution

Conjunctive Normal Form ( CNF -universal) - basically converting semantic to propositional logic

conjunction of disjunction of literal } caluses

![image-20220217170325632](/home/jojo/Documents/UNI/term2/images/image-20220217170325632.png)

![image-20220217170347336](/home/jojo/Documents/UNI/term2/images/image-20220217170347336.png)

![image-20220217170406951](/home/jojo/Documents/UNI/term2/images/image-20220217170406951.png)

Try and prove by contradiction the CNF 

Forward and backward chaining

google forward and backward chaining

### Pro's & cons of simple logic

- Propositional logic is **declarative**
- Propositional logic allows partial/disjunctive/negated (unlike most data structures and databases)
- Propositional logic is compositional
- Meaning in a propositional logic is context-independent
  - unlike natural language, where meaning depends on context
- Propositional logic has very limited expressive power

## First order logic

- first order logic (like natural logic)
  - Objects: people , houses , numbers , colours , baseball games , wars
  - Relations: red , round , prime, brother of, bigger than, part of , comes between....
  - Functions : father of, best friend , one more than, plus.....

### First Order Logic : Basic elements

![image-20220222143803267](/home/jojo/Documents/UNI/term2/images/image-20220222143803267.png)

### Atomic sentences

Atomic sentence = (predicate (term1,....term n)) | term1 = term2

Term = function(term1,...Termn) | constant or variable

E.G 

- Brother (KingJohn , RichardTheLionheart),
- greather_than(Length(LeftLegOf(Richard))),
- Length(LeftLegOf(KingJohn))

Complex sentences

- Complex sentences are made from atomic sentences are made from atomic sentences using connectives

![image-20220222144430251](/home/jojo/Documents/UNI/term2/images/image-20220222144430251.png)

### Truth in first-order logic

- Sentences are true with respect to a model and an interpretation
- Model contains objects (domain elements ) and relations among them
- Interpretation specifies referents for
  - constant symbols -> objects 
  - predicate symbols -> relations
  - function symbols -> functional relations
- An atomic sentence predicate (term1 ... term n) is true iff the objects referred to by term 1, .... termn are in the relation referred to by predicate.

![image-20220222144911713](/home/jojo/Documents/UNI/term2/images/image-20220222144911713.png)

### Universal qualification

![image-20220222145216348](/home/jojo/Documents/UNI/term2/images/image-20220222145216348.png)

### A common mistake to avoid

![image-20220222145233580](/home/jojo/Documents/UNI/term2/images/image-20220222145233580.png)

### Existential quantification

![image-20220222145253928](/home/jojo/Documents/UNI/term2/images/image-20220222145253928.png)

![image-20220222145330186](/home/jojo/Documents/UNI/term2/images/image-20220222145330186.png)

### Properties of quantifiers

![image-20220222145502317](/home/jojo/Documents/UNI/term2/images/image-20220222145502317.png)

### Uses of FOL

1. Expert Systems and Knowledge Engineering
   1. Formalisation of knowlege of human experts
2. Ontologies
   1. Formalisation of all general knowledge
3. Common sense knowledge
   1. Formalisation of practical knowledge about the world

## Machine Learning 

Arthur Samuel (1959): "Field of study that gives computers the ability to learn without being explicit programmed".

- Some tasks are very hard to define precisely
- For some tasks the programmer  does not have complete knowledge.
- For some tasks manually encoding problem knowledge is difficult , tedious and takes too long



### Types of learning tasks by feedback

- Supervised Machine Learning:
  - the computer is presented with example inputs and desired outputs
  - the goal is to learn a general rule to map inputs to outputs
- Unsupervised Machine Learning:
  - no desired outputs are given to the computer
  - goal to discovering hidden patterns in input data
- Reinforcement Learning
  - learning a task without being explicitly given example 
  - only an indirect reward (reinforcement is given for the overall task)
- Noisy Environments:
  - the data presented to computer are imprecise or contain errors

### Types of learning tasks by representation

- Propositional or first-order logic sentences
- If-then rules or decision trees
- Numerical weighs of functions . e.g. Neural Networks
- Probabilistic descriptions

#### Supervised Learning Tasks

- Regression - supervised
  - estimate parameters , e.g. weight vs height
- Classification - supervised 
  - estimate class - e.g. handwritten digit classification

#### Supervised  Learning  Overview

![image-20220224105142076](/home/jojo/Documents/UNI/term2/images/image-20220224105142076.png)



![image-20220224105356056](/home/jojo/Documents/UNI/term2/images/image-20220224105356056.png)

As a supervised classification problem classification problem

Start with training e.g. 6000 examples of each digit

- Can achieve testing error of 0.4%
- One of the commercial and widely used ML systems (for zip codes & checks)

- Example 2: face detection

- A supervised classification problem
- Need to classify an image iwndow into two classes:
  - faces vs non -face

- Classifier learnt from labelled data

#### Training data for frontal faces

- 5000 faces
  - All near frontal
  - Age , race , gender , lighting
- 10^* non faces
- faces are normalized 
  - scale , translation

#### Spam detection

- This is a classification problem 
- Task is to classify email into spam/non -spam
- Data xi is word count .
- Requires a learning system as "enemy" keeps innovating

#### Stock price predictions

- Task is to predict stock price at future date 
- This is a regression task , as the output is continuous

### Regression

- Suppose we are given a training set of N observations

![image-20220224110932770](/home/jojo/Documents/UNI/term2/images/image-20220224110932770.png)

- Regression problem is to estimate y(x) from this data.

### Polynomial curve fitting

- The green curve is the true function

![image-20220224111042878](/home/jojo/Documents/UNI/term2/images/image-20220224111042878.png)

- The data points are uniform in x but have noise in y.
- using the loss that function that measured the squared error in the prediction of y(x) from x. The loss for the red polynomial is the sum of the squared vertical errors.

![image-20220224111156707](/home/jojo/Documents/UNI/term2/images/image-20220224111156707.png)

![image-20220224111213639](/home/jojo/Documents/UNI/term2/images/image-20220224111213639.png)

### Overfitting

- test data : a different sample from the same true function

  ![image-20220224112342258](/home/jojo/Documents/UNI/term2/images/image-20220224112342258.png)

- training error goes to zero , but test error increases with M

### Goodness of Fit vs Model Complexity

- If the model has as many degrees of freedom as the data, it can fit the training data perfectly
- But the objective in ML is generalization
- Can expect a model generalize well if it explains training data well given the complexity of the model.

### prevention of overfitting

- Add more data than the model "complexity"
- For the 9th order polynomial

![image-20220224112617493](/home/jojo/Documents/UNI/term2/images/image-20220224112617493.png)

### Setting model parameters

Use a validation set:

### Divide the total dataset in three subsets

- Training data is used for learning the parameters of the model.
- Validation data is not used for learning but is used for deciding what type of model and what amount of regularization works best.
- Test data is used to get a final, unbiased estimate of how well the learning machine works.  the expectation of this estimate is to be worse than validation data.

### K nearest Neighbour (K-NN) Classifier

Algorithm 

- For each test point , x , to be classified , find the K nearest sample in the training data.
- Classify the point ,x, according to the majority vote of their class labels.

e.g. K = 3

![image-20220224115755420](/home/jojo/Documents/UNI/term2/images/image-20220224115755420.png)

![image-20220224115804547](/home/jojo/Documents/UNI/term2/images/image-20220224115804547.png)

![image-20220224115815564](/home/jojo/Documents/UNI/term2/images/image-20220224115815564.png)

![image-20220224115855152](/home/jojo/Documents/UNI/term2/images/image-20220224115855152.png)

### Effect of the K parameter

As K increases:

- Classification boundary vbecomes smoother
- Training error can increase

### Choose K by cross validation

- Split training data into training and validation
- Hold out validation data and measure error on this

Example : hand written digit recognition

- Minst data set
- Distance = raw pixel distance between images
- 60k training examples 
- 10 k testing examples
- k - NN gives 5% classification error

![image-20220224120820863](/home/jojo/Documents/UNI/term2/images/image-20220224120820863.png)

### K-NN Classification  - Advantages

- K-NN is a simple but effective classification procedure
- Applies to multi class classification
- Decision surfaces are non - linear 
- Quality of predictions automatically improves with more training data
- Only a single parameter, K , easily tuned by cross validation/.

### K-NN Classification  - Advantages

- defining nearest , and specify metrics
- Computational cost: must store and search through the entire training set at test time. Can alleviate this problem by thinning and use of efficient data structures like KD trees.

### Supervised Learning

learn a function from examples

f is the *unknown* **target function**

An *example* is a pair (x, f(x))

```
Problem:
find a hypothesis h
such that h ~ f
given a training set of examples
```

![image-20220304181156461](/home/jojo/Documents/UNI/term2/image-20220304181156461.png)

### Method

- Construct h to aggress with f on training set 
- h is consistent if it aggrees with f on all examples
- E.g. curve fitting:

![image-20220304181259292](/home/jojo/Documents/UNI/term2/image-20220304181259292.png)

![image-20220304181323808](/home/jojo/Documents/UNI/term2/image-20220304181323808.png)

### Supervised learning method

- Construct h to agree with f on training set
- h is consistent if it agrees with f on all examples
- E.g., curve fitting:

![image-20220304181431901](/home/jojo/Documents/UNI/term2/image-20220304181431901.png)

![image-20220304181611651](/home/jojo/Documents/UNI/term2/image-20220304181611651.png)

- Ockham's razor:

prefer the simplest hypothesis consistent with data

### Learning decision trees

#### Attribute-based representation

- Examples described by attribute values
- e.g. situations where I will / won't for a table

![image-20220304182318910](/home/jojo/Documents/UNI/term2/image-20220304182318910.png)

#### Decision trees

- One possible representation for hypotheses

- E.g. , here is the " true" tree for deviding whether to wait:

  ![image-20220304182415280](/home/jojo/Documents/UNI/term2/image-20220304182415280.png)

#### Expressiveness

- Decision trees can express any function of the input attributes

  ![image-20220304182509544](/home/jojo/Documents/UNI/term2/image-20220304182509544.png)

- Trivially , there is consistent decision tree for any training set with one path leaf for each example (unless f nondeterministic in x) but it probablly won't generalize to new examples
- Prefer to find more compact decision trees.

### Decision tree learning

Aim: find a small tree consistent with the training examples

~~~
function DTL(examples, attributes, default) returns a decision tree
if examples is empty then return default
else if all examples have the same classification then return the calssification
else if attributes is empty then return MODE(examples)
else
	best<- Choose-Attribute(attributes, examples)
	tree <- a new decision tree with root test best
	for each value vi of best do
		examples <- {elements of examples with best =vi}
		subtree <- DTL(examples, attributes - best,MODE(examples))
		add a branch to tree with label vi and subtree subtree
	return tree
~~~

### Choosing an attribute

- Idea: good attribute splits the examples into subsets that are ideally "all positiove" or "all negative"

![image-20220304223909945](/home/jojo/Documents/UNI/term2/image-20220304223909945.png)

### Using information theory

- To implement Choose - Attribute in the DTL algorithm

- Information Content (Entropy):

  ![image-20220304224415601](/home/jojo/Documents/UNI/term2/image-20220304224415601.png)

- • For a training set containing p positive examples and n negative examples

![image-20220304224438271](/home/jojo/Documents/UNI/term2/image-20220304224438271.png)

### Information gain

- A chosen attribute A divides the training set E into subsets E1, Ev according to their values of A , where A has v distinct values.

  ![image-20220304232853909](/home/jojo/Documents/UNI/term2/image-20220304232853909.png)

- Information Gain (IG) or reduction in entropy from the attribute test:

![image-20220304232929381](/home/jojo/Documents/UNI/term2/image-20220304232929381.png)

- Choose the attribute with the largest IG

For the training set p = n = 6, I(6/12,6/12)= 1 bit

Consider teh attributes Patrons and Type as well as others

![image-20220304233048477](/home/jojo/Documents/UNI/term2/image-20220304233048477.png)

## Neural Networks

-  Analogy to biological neural systems, the most
  robust learning systems we know.
- • Attempt to understand natural biological systems
  through computational modeling.
- • Massive parallelism allows for computational
  efficiency.
- • Help understand "distributed" nature of neural
  representations ;rather than "localist"
  representation) that allow robustness and graceful
  degradation.
- • Intelligent behavior as an "emergent" property of
  large number of simple units rather than from
  explicitly encoded symbolic rules and algorithms.

## Unsupervised Learning

Supervised learning: discover patterns in the data that relate data attributes with a GIVEN target class or value attribute.

- patterns are utilized to predict values of target attribute in future data instances

### Classification: 

desired outputs yi, are discrete class labels. The goal is to classify new inputs correctly

### Regression

The desired outputs yi , are continous valued. The goal is to predict the output accurately for new inputs

### Supervised learning vs. unsupervised learning

- Unsupervised learning: The data have NO GIVEN target attribute.
  - want to explore the data to find some intrinsic structures in them
- TO build a model or find useful representations of the data , for example:
  - Finding clusters
  - dimensionality reduction
  - finding good explanations (hidden causes) of the data
  - modeling the data density

### Why unsupervised learning?

- Supervised learning: labelled data {(x,y)} is very expensive to obtain 
- Unsupervised learning: unlabelled data {x} is much cheaper to obtain

![image-20220307092320644](/home/jojo/Documents/UNI/term2/image-20220307092320644.png)

![image-20220307092835153](/home/jojo/Documents/UNI/term2/image-20220307092835153.png)

### How Unsupervised Learning

- model an image as a point in a n dimensional space where grey scale of each pixel of the image represents a cooridinate value
- Plot all points in this space. Images referring to the same digit are more similar to each others. Points tend to form natural clusters.
- Label each cluster with the digit it represents

![image-20220307102610329](/home/jojo/Documents/UNI/term2/image-20220307102610329.png)

#### Example - Word clustering using HMM's

Input: raw text (Large data set)

Output:

![image-20220307094027149](/home/jojo/Documents/UNI/term2/image-20220307094027149.png)

Impact : used in many state of the art NLP systems

#### Example: Features learning using unsupervised neural networks

![image-20220307094152381](/home/jojo/Documents/UNI/term2/image-20220307094152381.png)

### Clustering

- Clustering is a technique for finding similarity groups in data, called clusters. i.e.

  - It groups data instances that are similar to (near) each other in one cluster and data instances that are very different (far away) from each other into different clusters.

    

#### Input : training set of input points 

![image-20220307102741973](/home/jojo/Documents/UNI/term2/image-20220307102741973.png)

#### Output: assignment of each point to a cluster

![image-20220307102842478](/home/jojo/Documents/UNI/term2/image-20220307102842478.png)

### Clustering as Optimisation Problem (K- means)

Setup:

- Each cluster k = 1,...,K is represented by

  ![image-20220307103237862](/home/jojo/Documents/UNI/term2/image-20220307103237862.png)

- Intuition : want each point

![image-20220307103300789](/home/jojo/Documents/UNI/term2/image-20220307103300789.png)

close to its assigned centroid

![image-20220307103315970](/home/jojo/Documents/UNI/term2/image-20220307103315970.png)

Objective function:

![image-20220307103333289](/home/jojo/Documents/UNI/term2/image-20220307103333289.png)

Need to choose centroids u and assignments z jointly

### K-means problem example in one-dimension

![image-20220307104759744](/home/jojo/Documents/UNI/term2/image-20220307104759744.png)

K-means heuristic algorithm

![image-20220307105038394](/home/jojo/Documents/UNI/term2/image-20220307105038394.png)

Key Idea: alternating minimisation

Solve hard problem by solving *two easy* problems

##### K-means algorithm step 1

![image-20220307105544889](/home/jojo/Documents/UNI/term2/image-20220307105544889.png)

##### K-means algorithm step 2

![image-20220307110324645](/home/jojo/Documents/UNI/term2/image-20220307110324645.png)

### K means algorithm

Objective:

~~~
min z min u Loss kmeans(z,u)
~~~

Algorithm: K-means

~~~
Initialize u1,...uK randomly
For t=1,...T:
Step 1: set assignements z given u
Step 2: set centroids u given z
~~~

### K-means algorithm simple example

Input : Dtrain = {0,2,10,12}

Output: K = 2 centroids u1,u2 £ R

![image-20220307110607674](/home/jojo/Documents/UNI/term2/image-20220307110607674.png)

![image-20220307110713512](/home/jojo/Documents/UNI/term2/image-20220307110713512.png)

### Local minima 

K-means is guaranteed to converge to a local minimum , but is not guarenteed to find the global minimum

Solutions:

- Run multiple times from a different random initializations
- Initialize with a heuristic (K-means ++)

### Strengths of k-means

- Strenghts:

  - Simple: easy to understand and to implement
  - Efficient : Time complexity : O(tkn),

  where n is the number of data points ,

  k is the number of clusters , and t is  the number of iterations

  - SInce both k and t are small. k - means is considered a linear algorithm

- It is the most popular clustering algorithm

### Weakness of k-means

- terminates at a local optimum. The global optimum is hard to find due to problem complexity
- is only applicable if the mean is defined , that is it works in euclidean spaces but does not generalise to more complex spaces
- The user needs to specify k.

## Natural Language Processing

### Goal:

build systems that can process human language



### History

- Rules based methods "60's to 90's": deep and narrow
- Statistical methods ("90's to today": shallow and broad
- What we after : deep and broad

#### Chomsky

- Assumption:  linguistic knowledge is inate
- Emphasis on explanation
- goal : simplicity of theory

#### Emprical methods

- Assumption: linguistic knowledge primarily derives from generalizations over experience
- Emphasis on data
- Primary goal: fact discovery

1960 & 1980 was "chomskyism"

### NLP Challenges

Ultimate goal - computers use NL effectively as humans do

Reading and writing text

- Abstracting 
- Monitoring
- Extraction into Database

Interactive Dialogue

Translation



### Human machine Interfaces

#### SRHDLU

![image-20220311110036372](/home/jojo/Documents/UNI/term2/image-20220311110036372.png)

#### LUNAR

NLP interface to database of analyses of APOLLO 11 moon rocks

give quantative data

#### Crucial  Flaws

- Usually small set of examples
- years of work to port to new applications and , often to extend coverage on a single application
- Very limited to anything other than English

### Paradigm Shift

Statistical Diagrams

- Communication for the speaker
  - Intention: Decide when and what info should be transmitted. need to require deduction about agent's goals and beliefs.
  - Generation: Translate the information to be communicated (in internal logical representation or "language of thought") into string of words in desired natural language
  - Synthesis: output the string in desired modality , text or speech

- Communication for the hearer
  - Perception - Map input modality to a string of words, e.g. optical character recognition (OCR) or speech recognition
  - Analysis: Determine the information content of the string
    - parsing syntax : decypher the syntax
    - interperting semantics : extract the meaning
    - pragmatic interpertation : does it make sense
  - Incoporation: Decide wheter or not to believe the content of the string and add it to the KB.

![image-20220311111104775](/home/jojo/Documents/UNI/term2/image-20220311111104775.png)

![image-20220311111233313](/home/jojo/Documents/UNI/term2/image-20220311111233313.png)

### Ambiguity

Natural language is highly ambiguous and needs to be disambiguated.

![image-20220311111641233](/home/jojo/Documents/UNI/term2/image-20220311111641233.png)

###  Ambiguity is Ubiquitos

![image-20220311111855614](/home/jojo/Documents/UNI/term2/image-20220311111855614.png)

![image-20220311112227154](/home/jojo/Documents/UNI/term2/image-20220311112227154.png)

![image-20220311112507264](/home/jojo/Documents/UNI/term2/image-20220311112507264.png)

### Why Ambiguous

![image-20220311112519810](/home/jojo/Documents/UNI/term2/image-20220311112519810.png)

![image-20220311112623906](/home/jojo/Documents/UNI/term2/image-20220311112623906.png)

### Resolving Ambiguity

![image-20220311112708993](/home/jojo/Documents/UNI/term2/image-20220311112708993.png)

### Automatic Learning Approach

![image-20220311112949258](/home/jojo/Documents/UNI/term2/image-20220311112949258.png)

![image-20220311114333990](/home/jojo/Documents/UNI/term2/image-20220311114333990.png)

![image-20220311114441743](/home/jojo/Documents/UNI/term2/image-20220311114441743.png)

![image-20220311114503785](/home/jojo/Documents/UNI/term2/image-20220311114503785.png)

![image-20220311114538493](/home/jojo/Documents/UNI/term2/image-20220311114538493.png)

![image-20220311114626884](/home/jojo/Documents/UNI/term2/image-20220311114626884.png)
